{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Classifier\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### HYPERPARAMETERS & OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA SELECTION ###\n",
    "bkg_filename = \"../background_Full_Test3.txt\"\n",
    "sig_filename = \"../signal_Full_Test3.txt\"\n",
    "drop_PHI_columns = True\n",
    "n_train = 25000\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "### OPTIONS ###\n",
    "savePlots = False\n",
    "plotsLocation = \"./\"\n",
    "\n",
    "### GAN ###\n",
    "epochs = 10000\n",
    "batch_size = 265\n",
    "loadNetworks = False\n",
    "\n",
    "    # Discriminator\n",
    "loadWeightsD = False\n",
    "\n",
    "    ## Generator\n",
    "noise_dim = 13\n",
    "loadWeightsG  = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Load data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BACKGROUND events: 54194\n",
      "46.13% = 25000 used for TRAINING\n",
      "46.13% = 25000 used for TESTING\n",
      "07.74% = 4194 unused\n",
      "Number of SIGNAL events: 52766\n",
      "47.38% = 25000 used for TESTING\n",
      "52.62% = 27766 unused\n",
      "\n",
      "\n",
      "Training sample size:  (25000, 11)\n",
      "Testing sample size:  (54194, 11)\n"
     ]
    }
   ],
   "source": [
    "bkg_all = pd.read_csv(bkg_filename, delimiter=' ', index_col=False)\n",
    "sig_all = pd.read_csv(sig_filename, delimiter=' ', index_col=False)\n",
    "\n",
    "def coordinate_change (df):\n",
    "    pt1 = np.sqrt(df['px1']**2 + df['py1']**2)\n",
    "    theta1 = np.arctan2(pt1,df['pz1'])\n",
    "    eta1 = -1 * np.log(np.tan(theta1/2))\n",
    "    phi1 = np.arctan2(df['py1'],df['px1'])\n",
    "    pt2 = np.sqrt(df['px2']**2 + df['py2']**2)\n",
    "    theta2 = np.arctan2(pt2,df['pz2'])\n",
    "    eta2 = -1 * np.log(np.tan(theta2/2))\n",
    "    phi2 = np.arctan2(df['py2'],df['px2'])\n",
    "    df['px1'] = pt1\n",
    "    df['py1'] = eta1\n",
    "    df['pz1'] = phi1\n",
    "    df['px2'] = pt2\n",
    "    df['py2'] = eta2\n",
    "    df['pz2'] = phi2\n",
    "    df.rename(columns={'px1':'$p_{t1}$', 'py1':'$\\eta_{1}$', 'pz1':'$\\phi_{1}$', 'px2':'$p_{t2}$',\n",
    "                       'py2':'$\\eta_{2}$', 'pz2':'$\\phi_{2}$', 'E1':'$E_1$', 'E2':'$E_2$', 'M1':'$M_1$',\n",
    "                       'M2':'$M_2$', 'M12':'$M_{1 2}$'}, inplace=True)\n",
    "    \n",
    "coordinate_change(sig_all)\n",
    "coordinate_change(bkg_all)\n",
    "\n",
    "if drop_PHI_columns:\n",
    "    sig_sel = sig_all.drop(columns = ['$\\phi_{1}$', '$\\phi_{2}$'])\n",
    "    bkg_sel = bkg_all.drop(columns = ['$\\phi_{1}$', '$\\phi_{2}$'])\n",
    "else:\n",
    "    sig_sel = sig_all\n",
    "    bkg_sel = bkg_all\n",
    "    \n",
    "data_header = list(sig_sel)\n",
    "\n",
    "sig_shuffled = shuffle(sig_sel)\n",
    "bkg_shuffled = shuffle(bkg_sel)\n",
    "\n",
    "n_bkg = len(bkg_sel.iloc[:,:0])\n",
    "n_sig = len(sig_sel.iloc[:,:0])\n",
    "f_s = n_train/n_sig\n",
    "f_b = n_train/n_bkg\n",
    "\n",
    "print (\"Number of BACKGROUND events:\", n_bkg)\n",
    "print (\"%0.2f%% = %d\" % (f_b*100, n_train), \"used for TRAINING\")\n",
    "print (\"%.2f%% = %d\" % (f_b*100, n_train), \"used for TESTING\")\n",
    "print (\"%05.2f%% = %d\" % ((1-2*f_b)*100, n_bkg - 2*n_train), \"unused\")\n",
    "print (\"Number of SIGNAL events:\", n_sig)\n",
    "print (\"%.2f%% = %d\" % (f_s*100, n_train), \"used for TESTING\")\n",
    "print (\"%.2f%% = %d\" % ((1-f_s)*100, n_sig - n_train), \"unused\")\n",
    "print (\"\\n\")\n",
    "\n",
    "sample_train = bkg_sel[:n_train]\n",
    "sample_test = pd.concat([bkg_sel.iloc[n_train:2*n_train], sig_sel.iloc[:n_train]])\n",
    "sample_test = sample_test.reset_index(drop=True)\n",
    "\n",
    "input_dim = sample_train.shape[1]\n",
    "\n",
    "sample_train = scaler.fit_transform(sample_train)\n",
    "sample_test = scaler.transform(sample_test)\n",
    "\n",
    "print('Training sample size: ',sample_train.shape)\n",
    "print('Testing sample size: ',sample_test.shape)\n",
    "\n",
    "sample_train_input, sample_train_valid = train_test_split(sample_train,test_size=0.2,random_state=13) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_hidden_1 (Dens (None, 20)                280       \n",
      "_________________________________________________________________\n",
      "discriminator_hidden_2 (Dens (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "discriminator_hidden_3 (Dens (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "discriminator_hidden_4 (Dens (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "discriminator_output (Dense) (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 977\n",
      "Trainable params: 977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if not(loadNetworks):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(20, input_shape=(input_dim,), activation='relu', name='discriminator_hidden_1'))\n",
    "    discriminator.add(Dense(20, activation='relu', name='discriminator_hidden_2'))\n",
    "    discriminator.add(Dense(10, activation='relu', name='discriminator_hidden_3'))\n",
    "    discriminator.add(Dense(5, activation='relu', name='discriminator_hidden_4'))\n",
    "    discriminator.add(Dense(2, activation='sigmoid', name='discriminator_output'))\n",
    "\n",
    "if loadWeightsD:\n",
    "    discriminator.load_weights(\"discriminator.h5\")\n",
    "    \n",
    "discriminator.summary()\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_hidden_1 (Dense)   (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "generator_hidden_2 (Dense)   (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "generator_hidden_3 (Dense)   (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "generator_output (Dense)     (None, 13)                208       \n",
      "=================================================================\n",
      "Total params: 883\n",
      "Trainable params: 883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if not(loadNetworks):\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(10, input_shape=(input_dim,), activation='relu', name='generator_hidden_1'))\n",
    "    generator.add(Dense(20 , activation='relu', name='generator_hidden_2'))\n",
    "    generator.add(Dense(15, activation='relu', name='generator_hidden_3'))\n",
    "    generator.add(Dense(input_dim, activation='sigmoid', name='generator_output'))\n",
    "\n",
    "    \n",
    "if loadWeightsG:\n",
    "    generator.load_weights(\"generator.h5\")    \n",
    "\n",
    "generator.summary()\n",
    "generator.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "generator_hidden_1 (Dense)   (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "generator_hidden_2 (Dense)   (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "generator_hidden_3 (Dense)   (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "generator_output (Dense)     (None, 13)                208       \n",
      "_________________________________________________________________\n",
      "discriminator_hidden_1 (Dens (None, 20)                280       \n",
      "_________________________________________________________________\n",
      "discriminator_hidden_2 (Dens (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "discriminator_hidden_3 (Dens (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "discriminator_hidden_4 (Dens (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "discriminator_output (Dense) (None, 2)                 12        \n",
      "=================================================================\n",
      "Total params: 1,860\n",
      "Trainable params: 883\n",
      "Non-trainable params: 977\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img_adv = Input(shape=(input_dim,))\n",
    "AM = Sequential()\n",
    "output_img_gen = generator.layers[3](generator.layers[2](generator.layers[1](generator.layers[0](input_img_adv))))\n",
    "output_img_adv = discriminator.layers[4](discriminator.layers[3](discriminator.layers[2](discriminator.layers[1](discriminator.layers[0](output_img_gen)))))\n",
    "\n",
    "AM = Model(input_img_adv, output_img_adv)\n",
    "\n",
    "AM.get_layer('discriminator_hidden_1').trainable = False\n",
    "AM.get_layer('discriminator_hidden_2').trainable = False\n",
    "AM.get_layer('discriminator_hidden_3').trainable = False\n",
    "AM.get_layer('discriminator_hidden_4').trainable = False\n",
    "AM.get_layer('discriminator_output').trainable = False\n",
    "\n",
    "AM.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "AM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(32, 13), b.shape=(13, 10), m=32, n=10, k=13\n\t [[{{node generator_hidden_1/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_generator_hidden_1_input_0_0/_67, generator_hidden_1/kernel/read)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b22e13751eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdata_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(32, 13), b.shape=(13, 10), m=32, n=10, k=13\n\t [[{{node generator_hidden_1/MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_generator_hidden_1_input_0_0/_67, generator_hidden_1/kernel/read)]]"
     ]
    }
   ],
   "source": [
    "train_steps = epochs\n",
    "discriminator_history = []\n",
    "adversarial_history = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(train_steps):\n",
    "            data_train = sample_train[np.random.randint(0,sample_train.shape[0], size=batch_size)]\n",
    "            noise = np.random.uniform(0, 1.0, size=[batch_size, noise_dim])\n",
    "            data_fake = generator.predict(noise)\n",
    "            x = np.concatenate((data_train, data_fake))\n",
    "            y = np.concatenate((np.tile([0,1], [batch_size,1]), (np.tile([1,0], [batch_size,1]))))\n",
    "            d_loss = discriminator.train_on_batch(x, y)\n",
    "            discriminator_history.append(d_loss)\n",
    "            noise = np.random.uniform(0, 1.0, size=[batch_size, noise_dim])\n",
    "            y = np.tile([0,1], [batch_size,1])\n",
    "            a_loss = AM.train_on_batch(noise, y)\n",
    "            adversarial_history.append(a_loss)\n",
    "            prc = (i/train_steps)*100\n",
    "            log_mesg = \"%.2f%%  %d [D loss: %f]\" % (prc, i, d_loss)\n",
    "            log_mesg = \"%s  [A loss: %f]\" % (log_mesg, a_loss)\n",
    "            print(log_mesg)\n",
    "            \n",
    "end = time.time()\n",
    "train_time += (end-start)\n",
    "n_cycle += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(train_steps)\n",
    "file_name = \"%dcylce\" % (n_cycle)\n",
    "\n",
    "lossFig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(epochs, discriminator_history, 'b', label='Discriminator loss')\n",
    "plt.plot(epochs, adversarial_history, 'r', label='Adversarial Model loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "lossFig.savefig(file_name + \"Loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg = np.repeat(0, 50000)\n",
    "sig = np.repeat(1, 25000)\n",
    "true = np.concatenate((bkg,sig))\n",
    "decoded_test = discriminator.predict(sample_test)\n",
    "decoded_train = discriminator.predict(sample_train)\n",
    "scores = np.concatenate((decoded_train,decoded_test))\n",
    "\n",
    "print (decoded_test)\n",
    "\n",
    "train_min = int(train_time) // 60\n",
    "train_sec = train_time - 60 * train_min\n",
    "time_stamp = \"Total CPU train time: %d'  %.1f''\" % (train_min, train_sec)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "                   \n",
    "fp, vp, thresholds = roc_curve(true,scores[:,0])\n",
    "roc_auc = auc(fp, vp)\n",
    "\n",
    "plt.plot(fp,vp,color='red',label='ROC curve %s (AUC = %0.4f)'%('AE',roc_auc))\n",
    "\n",
    "plt.xlabel('FP')\n",
    "plt.ylabel('TP')\n",
    "plt.plot([0, 1],[0, 1],\n",
    "         linestyle='--',color=(0.6, 0.6, 0.6),\n",
    "         label='Random guess')\n",
    "#plt.plot([0, 0, 1],[0, 1, 1],color='yellow',label='Idéal')\n",
    "plt.grid()\n",
    "plt.legend(loc=\"best\", title =time_stamp)\n",
    "plt.tight_layout()\n",
    "plt.savefig(file_name + \"ROC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveNetworks = True\n",
    "updateWeights = True\n",
    "\n",
    "if saveNetworks:\n",
    "    discriminator_json = discriminator.to_json()\n",
    "    with open(\"discriminator.json\", \"w\") as json_file:\n",
    "        json_file.write(discriminator_json)\n",
    "    generator_json = generator.to_json()\n",
    "    with open(\"generator.json\", \"w\") as json_file:\n",
    "        json_file.write(generator_json)\n",
    "\n",
    "if updateWeights:\n",
    "    discriminator.save_weights(\"discriminator.h5\")\n",
    "    generator.save_weights(\"generator.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
